# 异步多进程爬虫实现:

```python
from multiprocessing.dummy import Pool
import requests
# 创建线程池
pool = Pool(3)
# 准备网址列表
url_list = ['www.baidu.com','www.baidu.com','www.baidu.com']
# 创建操作函数
def get_requests(url):
    # 函数返回获取到的网页源码的数据
    return requests.get(url).text
# 将返回的页面源码数据接收
request_text_list = pool.map(get_requests,url_list)

```





# 单线程多任务异步协程:

### 正常协程实现:

```python
# 单协程写法

import asyncio
import time

async func(url):
    print('start')
    time.sleep(3)
    print('stop')
    
f = func(url)
task = asynic.ensure_future(f)
loop = asyncio.get_event_loop()
loop.run_until+complete(task)

# 多重协程写法
# 多线程时,需要设置线程挂起,参考下列代码
import asyncio

async func(url):
    print('start')
    # 在需要进行挂起的位置进行线程等待声明,即await
    await asyncio.sleep(3)
    print('stop')
    
    
urls = [
	'www.baidu.com',  
	'www.baidu.com', 
	'www.baidu.com', 
	'www.baidu.com', 
	'www.baidu.com'      
]
tasks = []
for url in urls:
    f = func(url)
    task = asynic.ensure_future(f)
    # 在创建好新的写成之后,将其添加到协程列表中
    tasks.append(task)
# 创建事件循环
loop = asyncio.get_event_loop()
# 将协程列表注册到事件循环中去
# 在注册列表类型时，需要甚至线程挂起，所以需要使用asyncio。wait进行特殊声明
loop.run_until+complete(asyncio.wait(tasks))


```



### 在使用协程进行网页爬取的时候，是无法使用await挂起的，也就是说，requests并不支持挂起：

### 在这个时候，需要使用支持await的请求模块。

### 也就是aiohttp。

```python
import aiohttp
import asyncio
import time

async def func(url):
    # 构建aiohttp请求
    # 每一次请求都需要对
    async with aiohttp.ClientSession() as a:
        async with await a.get(url=url) as response:
            result = await response.text()
            print(result)
            return result
urls = [
    'https://www.baidu.com',
    'https://www.baidu.com',
    'https://www.baidu.com'
]
tasks = []
for url in urls:
    f = func(url)
    task = asyncio.ensure_future(f)
    tasks.append(task)

loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))
```

在aiohttp中，和requests一样有get和post请求

这些请求的参数基本相同，

唯一不同的地方在于代理IP

在requests中，代理IP的类型是一个字典类型的数据，

但是在这里，代理IP的类型是一个字符串

同时参数名为proxy



