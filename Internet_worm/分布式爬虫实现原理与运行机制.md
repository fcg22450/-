[TOC]



# 分布式爬虫

## 什么是分布式爬虫:

- ##### 基于多台电脑组建一个分布式机群,然后让集群中的每一台电脑执行同一组程序,然后让他们对同一个网站的数据进行分布爬取

## 使用分布式爬虫的目的:

- ##### 提升数据爬取效率

## 如何实现分布式爬取:

- ##### 基于 scrapy + redis  的形式实现分布式爬虫

  - ##### scrapy结合scrapy-radis组件实现分布式

# 原生的scrapy是无法实现分布式爬虫的

- ##### 调度器无法实现共享,从而导致出现重复请求
- ##### 管道无法实现共享,不利于数据汇总 

# scrapy-redis组件的作用:

- ##### 提供可以被共享的调度器和管道

# 环境配置:

- ##### 安装redis数据库
- ##### pip install scrapy_redis

# 编码流程:

- ##### 创建一个工程

- ##### 创建一个爬虫文件

  - ###### spider 和 CrawlSpider 都可以使用

  - ###### 修改爬虫文件

    - 导入组件

      - ```python
        from scrapy_redis.spider import RedisSpider,RedisCrawlSpider
        ```

    - 将创建好的爬虫文件类的父类修改为新导入的scrapy_redis的子类

    - 将start_url替换掉,换成redis_key

  - ###### 修改settings

    - 指定管道

      - ```python
        # 开启可被共享的管道
        ITEM_PIPELINES = {
            'scrapy_redis.pipelines.RedisPipeline': 300
        }
        ```

    - 指定调度器

      - ```python
        # 指定使用可被共享的调度器
        # 增加了一个去重容器类的配置,作用使用 Redis的set集合来存储请求的指纹数据,从而实现请求去重的持久化
        # 对重复的请求对象去重
        DUPEFILTER_CLASS = 'scrapy_redis.dupefilter.RFPDupeFilter'
        # 使用scrapy_redis组件自己的调度器
        SCHEDULER = 'scrapy_redis.scheduler.Scheduler'
        # 配置调度器是否持久化,也就是说在爬虫运行结束之后要不要清除Redis中请求队列和去重指纹的set
        # 这样实际上已经实现了增量式,也就是说,再次运行的时候,将不会将已经获取过的数据重复的获取,
        # 增量式的概念就是:
            # 在创建爬虫之后,每次运行都会进行对比,紧紧对网站的更新内容进行爬取,而不会进行重复数据的重复获取,减少内存占用,同时也更加合理
        SCHEDULER_PERSIST = True
        ```

    - 置定redis服务

      - ```python
        # REDIS_HOST = 'redis 服务器地址'
        REDIS_HOST = '127.0.0.1'
        REDIS_PORT = 6379
        ```

      - redis配置文件redis.windows.conf

        - bind 127.0.0.1
          - 将这行代码注释掉
          - 接触127.0.0.1绑定,使其他电脑可以访问本机redis
        - protected-mode yes
          - 将yes改为no,关闭redis数据保护,关闭后允许其他电脑修改本机redis 数据库

      - 携带配置文件启动redis服务

        - redis-server ./redis.windows.conf

      - 启动redis客户端

      - 运行爬虫文件

        - cd ./spiders
        - scrapy runspider spiderName.py

      - 向调度器放入起始的     **url**     

        - 队列的位置
          - 队列存储在队列中
          - lpush key value
          - 按照爬虫中设定好的目标redis队列名称放入起始url
          - 当你把起始url放入之后,将会自动开始爬取

      - 爬虫文件运行之后将会把获取到的数据自动存储到redis中

      - 数据存储到 spiderName:items, response存储到 spiderName:response

      - 注意,在进行分布式的时候,redis 的ip地址必须保持精确,也就是说,想要做分布式最好使用一个内网连接机群

      

